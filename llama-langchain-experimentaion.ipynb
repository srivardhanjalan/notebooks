{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Background\n",
    "\n",
    "This is a experimentation setup to use LLaMA3 LLM and Langchain, to execute contextual queries with personal data and derive insights.\n",
    "\n",
    "##Steps at a Glance\n",
    "\n",
    "1. Setup LLaMA3 LLM Locally and Execute Test Queries\n",
    "2. Start Jupyter Notebooks locally\n",
    "3. Setup and Execute queries using Langchain\n",
    "4. Setup Conversation Chat History\n",
    "5. Download Cricket Data\n",
    "6. Read the Cricket Data using Langchain\n",
    "7. Create Vector Indexes for the Data \n",
    "8. Execute Queries\n",
    "9. Experimentation\n",
    "10. Reference Documentation\n",
    "\n",
    "##1. Setup LLaMA3 LLM Locally and Execute Test Queries\n",
    "\n",
    "Detailed Steps: [Running LLaMA3 on Mac, Windows & Linux](https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/Running_Llama3_Anywhere/Running_Llama_on_Mac_Windows_Linux.ipynb)\n",
    "\n",
    "####Use homebrew to install Ollama\n",
    "`brew install ollama`\n",
    "\n",
    "####Using Ollama pull the llama3.1 8B LLM and run it\n",
    "```\n",
    "ollama pull llama3.1\n",
    "ollama run llama3.1\n",
    "```\n",
    "\n",
    "####Execute Queries\n",
    "\n",
    "Queries can be executed using the Command Line, where LLaMA3 has started up and using Curl as shown below\n",
    "\n",
    "```\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"llama3.1\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"who wrote the book godfather?\"\n",
    "    }\n",
    "  ],\n",
    "  \"stream\": false\n",
    "}'\n",
    "```\n",
    "\n",
    "##2. Start Jupyter Notebooks locally\n",
    "```\n",
    "# clone git repo: https://github.com/fastai/fastsetup\n",
    "git clone https://github.com/fastai/fastsetup.git\n",
    "\n",
    "# run the following command to setup conda\n",
    "./fastsetup/setup-conda.sh\n",
    "\n",
    "# install and run Jupyter Notebook locally\n",
    "conda install jupyter\n",
    "jupyter notebook --no-browser\n",
    "```\n",
    "\n",
    "Copy paste the URL from the console in the web browser to access Jupyter Notebooks\n",
    "\n",
    "##3. Setup and Execute queries using Langchain\n",
    "\n",
    "Create a new file with the extension .ipynb, and use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain_community # used for interaction with Ollama agent to execute queries with llama3.1\n",
    "!pip install sentence-transformers # used to chunk large input files to enable creation of vector indexes\n",
    "!pip install faiss-cpu # the vector datastore used to store the vector index\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0) \n",
    "response = llm.invoke(\"who wrote the book godfather?\") \n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4. Setup Conversation Chat History\n",
    "\n",
    "We are able to execute queries with llama3.1 now, but llama currently does not have the ability to answer follow up questions as it does not remember the context of for the perviously asked questions. The following setup will setup a session store where we save the conversations with llama, and provide it the history of the conversation so that it can remember what we are talking about and answer questions in that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# store is a dictionary that maps session IDs to their corresponding chat histories.\n",
    "store = {}  # memory is maintained outside the chain\n",
    "\n",
    "\n",
    "# A function that returns the chat history for a given session ID.\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "#  Define a RunnableConfig object, with a `configurable` key. session_id determines thread\n",
    "config = {\"configurable\": {\"session_id\": \"1\"}}\n",
    "\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    llm,\n",
    "    get_session_history,\n",
    ")\n",
    "\n",
    "conversation.invoke(\n",
    "    \"who wrote the book godfather?\",  # input or query\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "conversation.invoke(\n",
    "    \"tell me more\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5. Download Cricket Data\n",
    "\n",
    "Download the data in JSON format following website: https://cricsheet.org/downloads/\n",
    "Data Schema Information: https://cricsheet.org/format/json/#introduction-to-the-json-format\n",
    "\n",
    "There are various examples on the web where data from PDFs and Text files are used to build vector indexes and used for querying. The following notebook has a good example of scraping a webpage and using its information to build vector indexes: https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/RAG/hello_llama_cloud.ipynb\n",
    "\n",
    "\n",
    "Using this data will enable the exploration of using structured and semi-structured data for creating vector indexes and experiment with the different kinds of queries that can be used to draw meaningful insights from this data.\n",
    "\n",
    "##6. Read the Cricket Data using Langchain\n",
    "\n",
    "The downloaded data is pretty large (2.8GB) which makes experimenttaion slow. Create a separate folder called `cricket-data-sample` and copy paste 5-10 files from the downloaded data. This will be used for the purposes of testing.\n",
    "\n",
    "Next, we will load the data as Langchain Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "# from langchain.document_loaders.json_loader import JSONLoader\n",
    "\n",
    "DRIVE_FOLDER = \"/Users/srjalan/personal/data/cricket_sample\"\n",
    "loader = DirectoryLoader(DRIVE_FOLDER, glob='**/*.json', show_progress=True, loader_cls=TextLoader)\n",
    "\n",
    "# loader = DirectoryLoader(DRIVE_FOLDER, glob='**/*.json', show_progress=True, loader_cls=JSONLoader, loader_kwargs = {'jq_schema':'.content'})\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "print(f'document count: {len(documents)}')\n",
    "print(documents[0] if len(documents) > 0 else None)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7. Create Vector Index for the Data\n",
    "\n",
    "First we will split the data into chunks using the `RecursiveCharacterTextSplitter`. There are multiple splitters that can be used to improve the effectiency of queries. The following article describes some of the different splitters: https://medium.com/@sushmithabhanu24/retrieval-in-langchain-part-2-text-splitters-2d8c9d595cc9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import bs4\n",
    "\n",
    "# Split the document into chunks with a specified chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Store the document into a vector store with a specific embedding model\n",
    "vectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##8. Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore2.as_retriever()\n",
    ")\n",
    "\n",
    "question = \"Whick teams did Australia play against?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9. Experimentation\n",
    "\n",
    "To improve the performance of the query system, I tried to the `RecursiveJsonSplitter`, but this has not given me the desired result I was hoping for yet. Continuing the research and play with configurations to find a good soluiton. The code below demonstrates the setup for the `RecursiveJsonSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Load the data as Json Objects into memory, instead of loading them as Langchain Documents\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Path to folder with JSON files\n",
    "folder_path = '/Users/srjalan/personal/data/cricket_sample'\n",
    "\n",
    "# List to store all data\n",
    "data = []\n",
    "\n",
    "# Load each JSON file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(folder_path, filename), 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            data.append(json_data)\n",
    "\n",
    "# Check sample data\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveJsonSplitter\n",
    "import bs4\n",
    "\n",
    "# Split the document into chunks with a specified max chunk size\n",
    "json_splitter = RecursiveJsonSplitter(max_chunk_size=1000)\n",
    "all_splits = json_splitter.create_documents(data)\n",
    "\n",
    "# Store the document into a vector store with a specific embedding model\n",
    "vectorstore = FAISS.from_documents(all_splits, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore2.as_retriever()\n",
    ")\n",
    "\n",
    "question = \"Whick teams did Australia play against?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##10. Reference Documentation\n",
    "\n",
    "1. [Local RAG Agent with LLaMA3 - Advanced Example](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb)\n",
    "2. https://python.langchain.com/docs/how_to/document_loader_json/\n",
    "3. https://medium.com/@sushmithabhanu24/retrieval-in-langchain-part-2-text-splitters-2d8c9d595cc9\n",
    "4. [How to split JSON data](https://python.langchain.com/docs/how_to/recursive_json_splitter/)\n",
    "5. [Cricket Data API](https://cricketdata.org)\n",
    "6. [Kaggle ICC Cricket Data](https://www.kaggle.com/datasets/mahendran1/icc-cricket)\n",
    "7. [Create Your Customized ChatBot with Your Data Using LangChain](https://blog.gopenai.com/create-your-customized-chatbot-with-your-data-using-langchain-a715ad50a34d)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
